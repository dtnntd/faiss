{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS: So sÃ¡nh cÃ¡c Index Types\n",
    "# Index Comparison with Comprehensive Visualizations\n",
    "\n",
    "Notebook nÃ y so sÃ¡nh:\n",
    "- Flat, IVF, PQ, IVF+PQ, HNSW\n",
    "- Accuracy, Speed, Memory\n",
    "- Trade-offs vÃ  use cases\n",
    "- Recommendations cho tá»«ng scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "from utils.data_generator import generate_random_vectors, generate_query_vectors\n",
    "from utils.benchmark import benchmark_index, get_index_size, compare_indexes\n",
    "from utils.visualization import plot_memory_usage, plot_index_comparison\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "print(\"âœ“ Imports hoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup vÃ  Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dimension = 128\n",
    "n_vectors = 100000\n",
    "n_queries = 200\n",
    "k = 10\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dimension: {dimension}\")\n",
    "print(f\"  Database: {n_vectors:,} vectors\")\n",
    "print(f\"  Queries: {n_queries}\")\n",
    "print(f\"  k: {k}\")\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "database_vectors = generate_random_vectors(n_vectors, dimension)\n",
    "query_vectors = generate_query_vectors(n_queries, dimension)\n",
    "\n",
    "print(f\"\\nâœ“ Data ready\")\n",
    "print(f\"  Database size: {database_vectors.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build All Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building all indexes...\\n\")\n",
    "\n",
    "indexes = {}\n",
    "build_times = {}\n",
    "\n",
    "# 1. Flat Index\n",
    "print(\"[1/5] Building Flat Index...\")\n",
    "start = time.time()\n",
    "index_flat = faiss.IndexFlatL2(dimension)\n",
    "index_flat.add(database_vectors)\n",
    "build_times['Flat'] = time.time() - start\n",
    "indexes['Flat'] = index_flat\n",
    "print(f\"  âœ“ Build time: {build_times['Flat']:.3f}s\\n\")\n",
    "\n",
    "# 2. IVF Index\n",
    "print(\"[2/5] Building IVF Index...\")\n",
    "nlist = int(np.sqrt(n_vectors))\n",
    "start = time.time()\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index_ivf.train(database_vectors)\n",
    "index_ivf.add(database_vectors)\n",
    "index_ivf.nprobe = 10\n",
    "build_times['IVF'] = time.time() - start\n",
    "indexes['IVF'] = index_ivf\n",
    "print(f\"  âœ“ Build time: {build_times['IVF']:.3f}s\")\n",
    "print(f\"  âœ“ nlist={nlist}, nprobe=10\\n\")\n",
    "\n",
    "# 3. PQ Index\n",
    "print(\"[3/5] Building PQ Index...\")\n",
    "m = 8  # number of sub-vectors\n",
    "nbits = 8  # bits per sub-vector\n",
    "start = time.time()\n",
    "index_pq = faiss.IndexPQ(dimension, m, nbits)\n",
    "index_pq.train(database_vectors)\n",
    "index_pq.add(database_vectors)\n",
    "build_times['PQ'] = time.time() - start\n",
    "indexes['PQ'] = index_pq\n",
    "print(f\"  âœ“ Build time: {build_times['PQ']:.3f}s\")\n",
    "print(f\"  âœ“ m={m}, nbits={nbits}\\n\")\n",
    "\n",
    "# 4. IVF+PQ Index\n",
    "print(\"[4/5] Building IVF+PQ Index...\")\n",
    "start = time.time()\n",
    "quantizer_ivfpq = faiss.IndexFlatL2(dimension)\n",
    "index_ivfpq = faiss.IndexIVFPQ(quantizer_ivfpq, dimension, nlist, m, nbits)\n",
    "index_ivfpq.train(database_vectors)\n",
    "index_ivfpq.add(database_vectors)\n",
    "index_ivfpq.nprobe = 10\n",
    "build_times['IVF+PQ'] = time.time() - start\n",
    "indexes['IVF+PQ'] = index_ivfpq\n",
    "print(f\"  âœ“ Build time: {build_times['IVF+PQ']:.3f}s\")\n",
    "print(f\"  âœ“ nlist={nlist}, m={m}, nprobe=10\\n\")\n",
    "\n",
    "# 5. HNSW Index\n",
    "print(\"[5/5] Building HNSW Index...\")\n",
    "M = 32\n",
    "start = time.time()\n",
    "index_hnsw = faiss.IndexHNSWFlat(dimension, M)\n",
    "index_hnsw.hnsw.efConstruction = 40\n",
    "index_hnsw.add(database_vectors)\n",
    "index_hnsw.hnsw.efSearch = 16\n",
    "build_times['HNSW'] = time.time() - start\n",
    "indexes['HNSW'] = index_hnsw\n",
    "print(f\"  âœ“ Build time: {build_times['HNSW']:.3f}s\")\n",
    "print(f\"  âœ“ M={M}, efConstruction=40, efSearch=16\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"All indexes built successfully!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Flat index as ground truth\n",
    "print(\"Getting ground truth from Flat index...\")\n",
    "gt_distances, gt_indices = index_flat.search(query_vectors, k)\n",
    "print(f\"âœ“ Ground truth ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark All Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark each index\n",
    "print(\"\\nBenchmarking all indexes...\\n\")\n",
    "\n",
    "results = {}\n",
    "memory_sizes = {}\n",
    "\n",
    "for name, index in indexes.items():\n",
    "    print(f\"Benchmarking {name}...\")\n",
    "    \n",
    "    # Benchmark\n",
    "    bench_results = benchmark_index(index, query_vectors, k, gt_indices)\n",
    "    \n",
    "    # Get memory size\n",
    "    size_info = get_index_size(index)\n",
    "    \n",
    "    # Store\n",
    "    results[name] = {\n",
    "        'recall': bench_results[f'recall@{k}'],\n",
    "        'qps': bench_results['qps'],\n",
    "        'search_time': bench_results['search_time'],\n",
    "        'build_time': build_times[name],\n",
    "        'size_mb': size_info['size_mb']\n",
    "    }\n",
    "    memory_sizes[name] = size_info['size_mb']\n",
    "    \n",
    "    print(f\"  Recall: {bench_results[f'recall@{k}']:.3f}\")\n",
    "    print(f\"  QPS: {bench_results['qps']:.1f}\")\n",
    "    print(f\"  Size: {size_info['size_mb']:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ“ Benchmarking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison table\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPREHENSIVE INDEX COMPARISON\".center(90))\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Index':<12} {'Recall':<10} {'QPS':<12} {'Build(s)':<10} {'Size(MB)':<10} {'Compression':<12}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "baseline_size = results['Flat']['size_mb']\n",
    "for name, data in results.items():\n",
    "    compression = baseline_size / data['size_mb']\n",
    "    print(f\"{name:<12} \"\n",
    "          f\"{data['recall']:<10.3f} \"\n",
    "          f\"{data['qps']:<12.1f} \"\n",
    "          f\"{data['build_time']:<10.3f} \"\n",
    "          f\"{data['size_mb']:<10.2f} \"\n",
    "          f\"{compression:<12.1f}x\")\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization - Multi-metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-panel comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "index_names = list(results.keys())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(index_names)))\n",
    "\n",
    "# Plot 1: Recall\n",
    "ax = axes[0, 0]\n",
    "recalls = [results[name]['recall'] for name in index_names]\n",
    "bars = ax.barh(index_names, recalls, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Recall@10', fontsize=12)\n",
    "ax.set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.axvline(0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "ax.legend()\n",
    "for i, (bar, val) in enumerate(zip(bars, recalls)):\n",
    "    ax.text(val + 0.02, i, f'{val:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: QPS\n",
    "ax = axes[0, 1]\n",
    "qps_values = [results[name]['qps'] for name in index_names]\n",
    "bars = ax.barh(index_names, qps_values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Queries Per Second', fontsize=12)\n",
    "ax.set_title('QPS Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "for i, (bar, val) in enumerate(zip(bars, qps_values)):\n",
    "    ax.text(val * 1.1, i, f'{val:.0f}', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Memory Size\n",
    "ax = axes[0, 2]\n",
    "sizes = [results[name]['size_mb'] for name in index_names]\n",
    "bars = ax.barh(index_names, sizes, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Memory Size (MB)', fontsize=12)\n",
    "ax.set_title('Memory Usage', fontsize=14, fontweight='bold')\n",
    "for i, (bar, val) in enumerate(zip(bars, sizes)):\n",
    "    ax.text(val + max(sizes)*0.02, i, f'{val:.1f}', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 4: Build Time\n",
    "ax = axes[1, 0]\n",
    "build_times_list = [results[name]['build_time'] for name in index_names]\n",
    "bars = ax.barh(index_names, build_times_list, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Build Time (seconds)', fontsize=12)\n",
    "ax.set_title('Build Time Comparison', fontsize=14, fontweight='bold')\n",
    "for i, (bar, val) in enumerate(zip(bars, build_times_list)):\n",
    "    ax.text(val + max(build_times_list)*0.02, i, f'{val:.2f}s', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 5: Recall vs QPS scatter\n",
    "ax = axes[1, 1]\n",
    "for i, name in enumerate(index_names):\n",
    "    ax.scatter(results[name]['qps'], results[name]['recall'], \n",
    "               s=300, color=colors[i], edgecolors='black', linewidth=2,\n",
    "               label=name, zorder=5)\n",
    "    ax.annotate(name, (results[name]['qps'], results[name]['recall']),\n",
    "                xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "ax.set_xlabel('QPS (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Recall@10', fontsize=12)\n",
    "ax.set_title('Recall vs QPS Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(0.9, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 6: Compression ratio\n",
    "ax = axes[1, 2]\n",
    "compressions = [baseline_size / results[name]['size_mb'] for name in index_names]\n",
    "bars = ax.barh(index_names, compressions, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Compression Ratio', fontsize=12)\n",
    "ax.set_title('Compression vs Flat', fontsize=14, fontweight='bold')\n",
    "ax.axvline(1.0, color='red', linestyle='--', alpha=0.5, label='Flat baseline')\n",
    "ax.legend()\n",
    "for i, (bar, val) in enumerate(zip(bars, compressions)):\n",
    "    ax.text(val + max(compressions)*0.02, i, f'{val:.1f}x', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ÄÃ£ lÆ°u: 03_comprehensive_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spider/Radar Chart - Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize metrics for radar chart (0-1 scale, higher is better)\n",
    "def normalize_metric(values, higher_is_better=True):\n",
    "    arr = np.array(values)\n",
    "    if not higher_is_better:\n",
    "        arr = 1 / (arr + 1e-10)  # Invert for metrics where lower is better\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min() + 1e-10)\n",
    "\n",
    "# Prepare data\n",
    "categories = ['Recall', 'Speed\\n(QPS)', 'Memory\\nEfficiency', 'Build\\nSpeed']\n",
    "n_cats = len(categories)\n",
    "\n",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Angles for each category\n",
    "angles = np.linspace(0, 2 * np.pi, n_cats, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Plot each index\n",
    "for i, name in enumerate(index_names):\n",
    "    values = [\n",
    "        results[name]['recall'],  # Higher is better\n",
    "        normalize_metric([results[n]['qps'] for n in index_names])[i],  # Higher is better\n",
    "        normalize_metric([results[n]['size_mb'] for n in index_names], False)[i],  # Lower is better\n",
    "        normalize_metric([results[n]['build_time'] for n in index_names], False)[i]  # Lower is better\n",
    "    ]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=name, markersize=8)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "# Fix axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
    "ax.grid(True)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title('Overall Index Performance (Normalized)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ÄÃ£ lÆ°u: 03_radar_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pareto Frontier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Pareto optimal indexes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Recall vs Speed\n",
    "ax = axes[0]\n",
    "for i, name in enumerate(index_names):\n",
    "    x = results[name]['qps']\n",
    "    y = results[name]['recall']\n",
    "    size = results[name]['size_mb']\n",
    "    \n",
    "    ax.scatter(x, y, s=size*5, alpha=0.6, color=colors[i], \n",
    "               edgecolors='black', linewidth=2, label=name)\n",
    "    ax.annotate(name, (x, y), xytext=(10, 5), \n",
    "                textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('QPS (Higher is Better)', fontsize=12)\n",
    "ax.set_ylabel('Recall@10 (Higher is Better)', fontsize=12)\n",
    "ax.set_title('Recall vs Speed\\n(Bubble size = Memory)', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add quadrant lines\n",
    "ax.axhline(0.9, color='red', linestyle='--', alpha=0.3, label='90% recall')\n",
    "ax.axvline(np.median(qps_values), color='blue', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 2: Memory vs Speed\n",
    "ax = axes[1]\n",
    "for i, name in enumerate(index_names):\n",
    "    x = results[name]['qps']\n",
    "    y = results[name]['size_mb']\n",
    "    recall = results[name]['recall']\n",
    "    \n",
    "    # Color by recall\n",
    "    color_val = plt.cm.RdYlGn(recall)\n",
    "    ax.scatter(x, y, s=300, alpha=0.7, color=color_val,\n",
    "               edgecolors='black', linewidth=2, label=name)\n",
    "    ax.annotate(name, (x, y), xytext=(10, 5),\n",
    "                textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('QPS (Higher is Better)', fontsize=12)\n",
    "ax.set_ylabel('Memory (MB) (Lower is Better)', fontsize=12)\n",
    "ax.set_title('Memory vs Speed\\n(Color = Recall)', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, \n",
    "                           norm=plt.Normalize(vmin=min(recalls), vmax=max(recalls)))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Recall@10', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_pareto_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ÄÃ£ lÆ°u: 03_pareto_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Decision Tree - Which Index to Use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision guide visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(0.5, 0.95, 'FAISS Index Selection Guide', \n",
    "        ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Decision tree text\n",
    "decision_text = \"\"\"\n",
    "ðŸ“Š DATASET SIZE:\n",
    "\n",
    "  < 100K vectors:\n",
    "    âœ“ Use FLAT for 100% accuracy\n",
    "    âœ“ Use HNSW for better speed, high accuracy (>95%)\n",
    "\n",
    "  100K - 1M vectors:\n",
    "    âœ“ Use HNSW for best quality\n",
    "    âœ“ Use IVF if memory is tight\n",
    "    âœ“ Use IVF+PQ if memory is critical\n",
    "\n",
    "  > 1M vectors:\n",
    "    âœ“ Use IVF or IVF+PQ\n",
    "    âœ“ PQ for extreme compression\n",
    "\n",
    "âš¡ PRIORITY:\n",
    "\n",
    "  Speed is critical:\n",
    "    1st: HNSW (if dataset fits)\n",
    "    2nd: IVF with low nprobe\n",
    "    3rd: IVF+PQ\n",
    "\n",
    "  Accuracy is critical:\n",
    "    1st: FLAT (100% accuracy)\n",
    "    2nd: HNSW (95-99% accuracy)\n",
    "    3rd: IVF with high nprobe (90-95%)\n",
    "\n",
    "  Memory is critical:\n",
    "    1st: PQ (50-100x compression)\n",
    "    2nd: IVF+PQ (20-50x compression)\n",
    "    3rd: IVF (similar to Flat)\n",
    "\n",
    "ðŸŽ¯ USE CASES:\n",
    "\n",
    "  FLAT:     Ground truth, evaluation, <100K vectors\n",
    "  IVF:      General purpose, 100K-10M vectors\n",
    "  PQ:       Memory constrained, can accept lower accuracy\n",
    "  IVF+PQ:   Large scale (>1M), production systems\n",
    "  HNSW:     High accuracy + speed, <10M vectors\n",
    "\n",
    "ðŸ’¡ TIPS:\n",
    "\n",
    "  â€¢ Start with IVF for prototyping\n",
    "  â€¢ Use FLAT to establish baseline\n",
    "  â€¢ Combine IVF+PQ for production scale\n",
    "  â€¢ HNSW for premium accuracy requirements\n",
    "  â€¢ Always benchmark on your actual data!\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.85, decision_text, \n",
    "        ha='left', va='top', fontsize=11, family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_decision_guide.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ÄÃ£ lÆ°u: 03_decision_guide.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary vÃ  Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*90)\n",
    "print(\"FINAL SUMMARY AND RECOMMENDATIONS\".center(90))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Find best for each metric\n",
    "best_recall = max(results.items(), key=lambda x: x[1]['recall'])\n",
    "best_speed = max(results.items(), key=lambda x: x[1]['qps'])\n",
    "best_memory = min(results.items(), key=lambda x: x[1]['size_mb'])\n",
    "best_build = min(results.items(), key=lambda x: x[1]['build_time'])\n",
    "\n",
    "print(f\"\\nðŸ† WINNERS:\")\n",
    "print(f\"  Best Recall:  {best_recall[0]:<10} ({best_recall[1]['recall']:.3f})\")\n",
    "print(f\"  Best Speed:   {best_speed[0]:<10} ({best_speed[1]['qps']:.0f} QPS)\")\n",
    "print(f\"  Best Memory:  {best_memory[0]:<10} ({best_memory[1]['size_mb']:.2f} MB)\")\n",
    "print(f\"  Best Build:   {best_build[0]:<10} ({best_build[1]['build_time']:.3f}s)\")\n",
    "\n",
    "# Calculate overall score (weighted)\n",
    "print(f\"\\nðŸ“Š OVERALL SCORE (weighted):\")\n",
    "print(f\"  Formula: 0.4*Recall + 0.3*Speed + 0.2*Memory + 0.1*Build\")\n",
    "print()\n",
    "\n",
    "scores = {}\n",
    "for name in index_names:\n",
    "    # Normalize metrics\n",
    "    norm_recall = results[name]['recall']\n",
    "    norm_speed = results[name]['qps'] / max(r['qps'] for r in results.values())\n",
    "    norm_memory = 1 - (results[name]['size_mb'] / max(r['size_mb'] for r in results.values()))\n",
    "    norm_build = 1 - (results[name]['build_time'] / max(r['build_time'] for r in results.values()))\n",
    "    \n",
    "    score = 0.4*norm_recall + 0.3*norm_speed + 0.2*norm_memory + 0.1*norm_build\n",
    "    scores[name] = score\n",
    "    print(f\"  {name:<10}: {score:.3f}\")\n",
    "\n",
    "best_overall = max(scores.items(), key=lambda x: x[1])\n",
    "print(f\"\\n  ðŸ¥‡ Overall winner: {best_overall[0]} (score: {best_overall[1]:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ KEY TAKEAWAYS:\")\n",
    "print(f\"  1. FLAT: Perfect accuracy but doesn't scale\")\n",
    "print(f\"  2. IVF: Great balance for most use cases\")\n",
    "print(f\"  3. HNSW: Best quality when dataset fits in memory\")\n",
    "print(f\"  4. PQ/IVF+PQ: For extreme scale and memory constraints\")\n",
    "print(f\"  5. Always benchmark on YOUR data and requirements!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
